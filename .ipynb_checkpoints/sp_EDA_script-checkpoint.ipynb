{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'sp_analysis_script.ipynb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-37e9e40da3bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mexecute_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sp_analysis_script.ipynb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-37e9e40da3bc>\u001b[0m in \u001b[0;36mexecute_notebook\u001b[0;34m(nbfile)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexecute_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'sp_analysis_script.ipynb'"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import matplotlib as mpl\n",
    "# from matplotlib.pyplot import show\n",
    "# from scipy.stats import chi2_contingency\n",
    "%matplotlib inline\n",
    "\n",
    "import io\n",
    "# from nbformat import current\n",
    "\n",
    "def execute_notebook(nbfile):\n",
    "    with io.open(nbfile) as f:\n",
    "        nb = read(f, 'json')\n",
    "    ip = get_ipython()\n",
    "    for cell in nb.worksheets[0].cells:\n",
    "        if cell.cell_type != 'code':\n",
    "            continue\n",
    "        ip.run_cell(cell.input)\n",
    "execute_notebook(\"sp_analysis_script.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summary of the data, numbers of valid/complete/incomplete sessions\n",
    "# Valid sessions are those who accurately labeld at least 4 out of 5 honeypot pairs\n",
    "\n",
    "num_total_sessions = 5156\n",
    "num_comlete_sessions = 2740\n",
    "num_valid_sessions = 2602\n",
    "num_incomplete_sessions = 2416\n",
    "\n",
    "print '%d Total sessions'%num_total_sessions\n",
    "print '%d Complete sessions'%num_comlete_sessions\n",
    "print '%d Valid sessions'%num_valid_sessions\n",
    "print '%d Incomplete'%num_incomplete_sessions\n",
    "\n",
    "# Pie Chart to show the above numbers\n",
    "cmap = plt.cm.prism\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.suptitle(\"SpeedPerception Data of %d samples\"%(num_total_sessions), fontsize=30)\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.pie(\n",
    "    [num_comlete_sessions, num_incomplete_sessions],\n",
    "    labels=['complete', 'incomplete'],\n",
    "    colors=['darksalmon', 'darkkhaki'],\n",
    "    startangle=90,\n",
    "    shadow=True,\n",
    "    explode=(0, 0.03),\n",
    "    autopct='%1.1f%%'\n",
    ")\n",
    "ax1.axis('equal')\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.pie(\n",
    "    [num_comlete_sessions-num_valid_sessions, num_comlete_sessions],\n",
    "    labels=['invalid', 'valid'],\n",
    "    colors=['darksalmon', 'darkkhaki'],\n",
    "    startangle=120,\n",
    "    shadow=True,\n",
    "    explode=(0, 0.1),\n",
    "    autopct='%1.1f%%'\n",
    ")\n",
    "ax2.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read valid session result data\n",
    "df_valid = pd.read_csv('./data/sp_csv3.csv')\n",
    "print 'There are %d valid votes'%(len(df_valid))\n",
    "print 'There are %d valid sessions'%(len(df_valid)/16.)\n",
    "df_valid.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of valid votes on each video pair\n",
    "pair_count = df_valid.groupby(['pairID']).agg(['count'])\n",
    "plt.figure(figsize=(20,6))\n",
    "s = pd.Series(pair_count['vote']['count'].values)\n",
    "g = s.plot(kind='bar' )\n",
    "g.set(xticklabels=[], xlabel='160 Video Pairs', ylabel='Number of Votes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute majority vote for each pair\n",
    "max_vote = {}\n",
    "for i, id_ in enumerate(df_valid['pairID'].unique()):\n",
    "    one_pair = df_valid.loc[df_valid['pairID']==id_]\n",
    "    tmp = one_pair.groupby(['vote']).count()['sessionID']\n",
    "    if len(tmp) < 3:\n",
    "        for v in ['equal', 'left', 'right']:\n",
    "            if not v in tmp.index.values:\n",
    "                tmp.loc[v] = 0        \n",
    "    count0, count1, count2 = tmp[0], tmp[1], tmp[2]\n",
    "    new_l = sorted([(count1, 'pick1'), (count2, 'pick2'), (count0, 'pick0')], key=lambda x: x[0])\n",
    "    max_vote[id_] = int(new_l[2][1][-1])\n",
    "max_pick = []\n",
    "for _id in df_valid['pairID'].ravel():\n",
    "    max_pick.append(max_vote[_id])\n",
    "df_valid['majority_pick'] = max_pick  \n",
    "df_valid.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_perf = pd.read_csv('./data/sp_csv1.csv')\n",
    "df_perf = df_perf[[\n",
    "        '_TTFB',\n",
    "        '_firstPaint',\n",
    "        '_domContentLoadedEventStart',\n",
    "        '_domContentLoadedEventEnd',\n",
    "        '_render',\n",
    "        '_SpeedIndex',\n",
    "        '_loadTime',\n",
    "        '_lastVisualChange',\n",
    "        '_visualComplete',\n",
    "        'uid']]\n",
    "df_pair = pd.read_csv('./data/sp_csv2.csv')\n",
    "mapping = {'equal': 0, 'left': 1, 'right': 2}\n",
    "df_valid = df_valid.replace({'vote': mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge performance metrics with user sessions\n",
    "# map performance metrics to picks\n",
    "\n",
    "for col in df_perf.columns.tolist()[:-1]:\n",
    "    d = ([],[])\n",
    "    for i in range(len(df_pair)):\n",
    "        wpt1, wpt2 = df_pair['uid_left'][i], df_pair['uid_right'][i]\n",
    "        d[0].append(df_perf[df_perf['uid']==wpt1][col].values[0])\n",
    "        d[1].append(df_perf[df_perf['uid']==wpt2][col].values[0])\n",
    "    df_pair[col[1:] + \"_%d\"%(1)] = d[0]\n",
    "    df_pair[col[1:] + \"_%d\"%(2)] = d[1]\n",
    "pair_perf = pd.merge(df_valid, df_pair, on=['pairID'])\n",
    "\n",
    "d_diff = {}\n",
    "pair_perf_m = pair_perf.iloc[:,range(8,26)]\n",
    "\n",
    "for col in df_perf.columns.tolist()[:-1]:\n",
    "    l1 = pair_perf_m[col[1:] + '_1'].ravel()\n",
    "    l2 = pair_perf_m[col[1:] + '_2'].ravel()\n",
    "    pick = pick_one(l1, l2)\n",
    "    d_diff[col[1:] + \"_pick\"] = pick\n",
    "\n",
    "perf_pick = pd.DataFrame(d_diff)\n",
    "df_valid_perf = pd.concat([pair_perf[['pairID', 'vote', 'majority_pick']],perf_pick], axis=1)\n",
    "df_pair.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Percentage match of each performance based pick to population pick\n",
    "target1 = 'Population'\n",
    "percentage_match(df_valid_perf, target1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Percentage match of each performance based pick to majority pick\n",
    "# unique_pair_perf = df_valid_perf.drop_duplicates(['pairID'], keep='first')\n",
    "# target2 = 'Majrotiy'\n",
    "# percentage_match(unique_pair_perf, target2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # cramer's V\n",
    "# association based on chi-square test statistics \n",
    "# WRT to population\n",
    "d_cat = {}\n",
    "for col in df_valid_perf.columns.tolist():\n",
    "    d_cat[col] = map({0: 'euqal', 1: 'left', 2: 'right'}.get, df_valid_perf[col])\n",
    "chiSquare_association(d_cat, target1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WRT to majority\n",
    "# chiSquare_association(d_cat, target2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as scp\n",
    "\n",
    "d_pair_entropy = {}\n",
    "d_colMetric_delta = {}\n",
    "\n",
    "col_m = df_perf.columns.tolist()[:-1] \n",
    "pearson_corr = []\n",
    "spearman_corr = []\n",
    "\n",
    "for c in col_m:\n",
    "    dd = {}\n",
    "    for i, id_ in enumerate(pair_perf['pairID'].unique()):\n",
    "        tmp = pair_perf.loc[pair_perf['pairID'] == id_]\n",
    "        result_list = tmp['vote'].tolist()\n",
    "        ent = entropy(result_list)\n",
    "        d_pair_entropy[id_] = ent\n",
    "        c1 = c[1:]+'_1'\n",
    "        c2 = c[1:]+'_2'\n",
    "        v1 = tmp[c1].unique()[0]\n",
    "        v2 = tmp[c2].unique()[0]\n",
    "        delta = (v1 - v2)/((v1+v2)/2.)\n",
    "        dd[id_] = abs(delta)\n",
    "        d_colMetric_delta[c] = dd\n",
    "    a1 = pd.Series(d_pair_entropy)\n",
    "    a2 = pd.Series(d_colMetric_delta[c])\n",
    "    pearson_corr.append((c, scp.pearsonr(a1, a2)[0]))\n",
    "    spearman_corr.append((c, scp.spearmanr(a1, a2)[0]))\n",
    "    \n",
    "df_corr= pd.DataFrame({'Metric':[x[0][1:] for x in spearman_corr]*2, \n",
    "              'Corr':[x[1] for x in pearson_corr]+[x[1] for x in spearman_corr],\n",
    "              'Method':['Pearson']*(len(col_m))+['Spearman']*(len(col_m))\n",
    "             })\n",
    "df_corr = df_corr.sort(['Corr'], ascending=True)\n",
    "df_corr = df_corr.fillna(df_corr.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "s = pd.Series(d_pair_entropy.values())\n",
    "g = s.plot(kind='bar' )\n",
    "# plt.xticks(rotation=40)\n",
    "g.set(xticklabels=[], xlabel='%d Video Pairs'%160, ylabel='Entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins = [0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1]\n",
    "d_entropy_count = {}\n",
    "for i in range(len(bins) - 1):\n",
    "    d_entropy_count['%.1f_%.1f'%(bins[i], bins[i+1])] = 0\n",
    "c_list = []\n",
    "for v in d_pair_entropy.values():\n",
    "    for i in range(len(bins) - 1):\n",
    "        if bins[i] <= v < bins[i+1]:\n",
    "            d_entropy_count['%.1f_%.1f'%(bins[i], bins[i+1])] += 1\n",
    "for key, value in d_entropy_count.items():\n",
    "    c_list.append((key, value))\n",
    "c_list = sorted(c_list, key=lambda x: x[0])\n",
    "plot_bar(c_list, 'Count of Videos in Different Entropy Intervals', 'count','bins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.title('Correlation of Result Entropy and Visual Metrics Delta', size=20)\n",
    "plt.gca()\n",
    "ax = sns.barplot(x=\"Metric\", y=\"Corr\", hue=\"Method\", data=df_corr, palette=\"muted\")\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    if p.get_y() < 0:\n",
    "        h = 0.01\n",
    "        ax.text(p.get_x()+0.1, h, '%1.2f'%(p.get_y()))\n",
    "    else:\n",
    "        ax.text(p.get_x()+0.1, height+0.01, '%1.2f'%(height))\n",
    "ax.set(xlabel = '', ylabel='Correlation')\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "pair_perf_copy = pair_perf.copy()\n",
    "GIF_DURATION_STRETCH_FACTOR = 1.09412199387\n",
    "offset = 800\n",
    "\n",
    "pair_perf_copy['TimeToClick_InMS'] = GIF_DURATION_STRETCH_FACTOR * pair_perf_copy['TimeToClick_InMS']\n",
    "visual_metrics_median = {}\n",
    "for metric in col_m:\n",
    "    visual_metrics_median[metric[1:]] = df_perf[metric].median()\n",
    "median_sorted_metrics = sorted(visual_metrics_median.items(), key = operator.itemgetter(1))\n",
    "print median_sorted_metrics\n",
    "\n",
    "sorted_visual_metrics = [i[0] for i in median_sorted_metrics]\n",
    "print '************** Timing Analysis ****************'\n",
    "### 3.1 : Count of votes made before/after/in-between the visual metrics of a pair\n",
    "### Pie chart to show the timing numbers\n",
    "cmap = plt.cm.prism\n",
    "colors = cmap(np.linspace(0., 1., 2))\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "# fig.suptitle(\"SpeedPerception Response Timing Data of %d samples\"%len(df_timing))\n",
    "### Charts in the order of sorted visual metrics\n",
    "chart_count = 0\n",
    "before_both_metrics = []\n",
    "after_both_metrics = []\n",
    "in_between_metrics = []\n",
    "for metric in sorted_visual_metrics:\n",
    "    chart_count = chart_count + 1\n",
    "\n",
    "    d_before_both_metrics = pair_perf_copy.loc[\n",
    "            (pair_perf_copy['TimeToClick_InMS'] - offset < pair_perf_copy[metric + '_%d'%1])\n",
    "            & \n",
    "            (pair_perf_copy['TimeToClick_InMS'] - offset< pair_perf_copy[metric + '_%d'%2])\n",
    "        ]\n",
    "\n",
    "    timings_before_both_metrics = d_before_both_metrics.count()[0]\n",
    "    print 'Number of tests with view duration less than both %s: %d'%(metric, timings_before_both_metrics)\n",
    "\n",
    "    d_after_both_metrics = pair_perf_copy.loc[\n",
    "            (pair_perf_copy['TimeToClick_InMS'] - offset> pair_perf_copy[metric + '_%d'%1])\n",
    "            & \n",
    "            (pair_perf_copy['TimeToClick_InMS'] - offset> pair_perf_copy[metric + '_%d'%2])\n",
    "        ]\n",
    "    timings_after_both_metrics = d_after_both_metrics.count()[0]\n",
    "    print 'Number of tests with view duration greater than both %s: %d'%(metric, timings_after_both_metrics)\n",
    "\n",
    "    timings_in_between_metrics = (len(pair_perf_copy) - timings_before_both_metrics - timings_after_both_metrics)\n",
    "    print 'Number of tests with duration lying in between both %s: %d'%(metric, timings_in_between_metrics)\n",
    "    print \n",
    "    \n",
    "    before_both_metrics.append(timings_before_both_metrics/float(len(pair_perf_copy)))\n",
    "    after_both_metrics.append(timings_after_both_metrics/float(len(pair_perf_copy)))\n",
    "    in_between_metrics.append(timings_in_between_metrics/float(len(pair_perf_copy)))\n",
    "    ax = fig.add_subplot(3,3, chart_count)\n",
    "    ax.set_title(metric)\n",
    "    ax.pie(\n",
    "        [timings_before_both_metrics, timings_after_both_metrics, timings_in_between_metrics],\n",
    "        labels= ['Before', 'After', 'In Between'],\n",
    "        colors= ['darksalmon', 'darkkhaki', 'darkturquoise'],\n",
    "        startangle= 200,\n",
    "        shadow=True,\n",
    "        explode=(0, 0.01, 0.01),\n",
    "        autopct='%1.1f%%',\n",
    "        labeldistance= 1.1\n",
    "    )\n",
    "\n",
    "    ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_context({\"figure.figsize\": (12, 4)})\n",
    "total_metrics = [1]*len(sorted_visual_metrics)\n",
    "ttd_timeline = {\n",
    "    'percentage': in_between_metrics+after_both_metrics+before_both_metrics,\n",
    "    'click_time': ['in_between_metrics']*len(sorted_visual_metrics) +\n",
    "    ['after_both_metrics']*len(sorted_visual_metrics) + ['before_both_metrics']*len(sorted_visual_metrics),\n",
    "    'metrics_timeline': sorted_visual_metrics * 3\n",
    "               }\n",
    "# pd.DataFrame(ttd_timeline).plot(kind='barh', stacked=True)\n",
    "ttd_df = pd.DataFrame(ttd_timeline)\n",
    "sns.pointplot(x='metrics_timeline', y='percentage',hue='click_time', data= ttd_df,\n",
    "             palette={'in_between_metrics':'darkturquoise', \n",
    "                     'after_both_metrics':'darkkhaki',\n",
    "                     'before_both_metrics':'darksalmon'})\n",
    "plt.ylabel('percentage')\n",
    "plt.title('TimeToClick_pointplot')\n",
    "plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmap = plt.cm.prism\n",
    "colors = cmap(np.linspace(0., 1., 2))\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "chart_count = 0\n",
    "for metric in col_m:\n",
    "    data = df_perf[metric]\n",
    "    chart_count = chart_count + 1\n",
    "    ax = fig.add_subplot(3, 3, chart_count)\n",
    "    ax.set_title(metric)\n",
    "    ax.hist(data, bins = 100)\n",
    "    ax.axvline(data.median(), color = 'red', linestyle = 'dashed', linewidth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
